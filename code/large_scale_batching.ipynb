{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFSzlEmWhnTX"
      },
      "source": [
        "# large scale batching\n",
        "Due to our large paragraph and openai rate limits we had to batch our use of the batches API, the following code does that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## install libraries and import them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjhKqWMkhKUg"
      },
      "outputs": [],
      "source": [
        "%pip install tiktoken\n",
        "%pip install openai\n",
        "%pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikpPai8tf94U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from itertools import permutations\n",
        "from unidecode import unidecode\n",
        "import concurrent.futures\n",
        "\n",
        "client = OpenAI(api_key=\"<api_key>\")\n",
        "\n",
        "\n",
        "\n",
        "existing_batches = list(client.batches.list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup tokenizers and input limits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eob9zRiogVD0"
      },
      "outputs": [],
      "source": [
        "token_limits ={\n",
        "    \"gpt-4o-2024-08-06\": {\n",
        "        \"tokens\":200000000,\n",
        "        'tokenizer': tiktoken.encoding_for_model(\"gpt-4o\"),\n",
        "    },\n",
        "    \"gpt-4o-mini\": {\n",
        "        \"tokens\":1000000000,\n",
        "        'tokenizer': tiktoken.encoding_for_model(\"gpt-4o-mini\"),\n",
        "    },\n",
        "    \"gpt-3.5-turbo-1106\": {\n",
        "        \"tokens\":1000000000,\n",
        "        'tokenizer': tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## load the claude paraphrases and original texts and prepare for batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA3jUQHkgU6H"
      },
      "outputs": [],
      "source": [
        "huge_df = pd.read_csv('/content/drive/MyDrive/oreilly_january/huge_df.csv')\n",
        "huge_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkmXjUmTgTRY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "from unidecode import unidecode\n",
        "\n",
        "\n",
        "def build_question(row, model):\n",
        "    \"\"\"\n",
        "    Constructs a formatted question and prepares a request for OpenAI's api\n",
        "    to run answer a decop quiz.\n",
        "    \"\"\"\n",
        "    extra_prompt = (\n",
        "        f\"Question: Which of the following passages is verbatim from the \\\"{row['Title']}\\\" \"\n",
        "        f\"book by {row['Formatted Author']}?\\nOptions:\\n\"\n",
        "    )\n",
        "    prompt = (\n",
        "        extra_prompt\n",
        "        + 'A. ' + row['Example_A'] + '\\n'\n",
        "        + 'B. ' + row['Example_B'] + '\\n'\n",
        "        + 'C. ' + row['Example_C'] + '\\n'\n",
        "        + 'D. ' + row['Example_D'] + '\\n'\n",
        "        + 'Answer: '\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"custom_id\": f\"{model}|{row['Snippet_ID']}.{row['Permutation_Number']}\",\n",
        "        \"method\": \"POST\",\n",
        "        \"url\": \"/v1/chat/completions\",\n",
        "        \"body\": {\n",
        "            \"model\": model,\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful assistant. You must answer using only the provided \"\n",
        "                        \"options A, B, C or D, you may not decline to answer.\"\n",
        "                    )\n",
        "                },\n",
        "                # unidecode is neccessary to normalize the unicode characters in the original text\n",
        "                {\"role\": \"user\", \"content\": unidecode(prompt)},\n",
        "            ],\n",
        "            \"max_tokens\": 1,\n",
        "            \"temperature\": 0,\n",
        "            \"seed\": 2319,\n",
        "            \"logprobs\": True,\n",
        "            # This is to make A,B,C and D more likely\n",
        "            \"logit_bias\": {32: +100, 33: +100, 34: +100, 35: +100},\n",
        "            \"top_logprobs\": 20\n",
        "        },\n",
        "    }\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model in [\"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"gpt-3.5-turbo-1106\"]:\n",
        "    # if execution is interrupted start from where the batches left off\n",
        "    # # Figure out which rows are excluded for this model\n",
        "    # model_dones = [done for done in dones if done['model'] == model]\n",
        "    # excluded_ids = set(f\"{d['question_number']}.{d['permutation_number']}\" for d in model_dones)\n",
        "    excluded_ids = set()\n",
        "\n",
        "    # Function to check if a row should be excluded\n",
        "    def _excluded(x):\n",
        "        return f\"{x['Snippet_ID']}.{x['Permutation_Number']}\" in excluded_ids\n",
        "\n",
        "    # Filter out excluded rows\n",
        "    if hasattr(huge_df, 'apply'):\n",
        "        # If huge_df is a pandas DataFrame\n",
        "        model_df = huge_df[~huge_df.apply(_excluded, axis=1)]\n",
        "    else:\n",
        "        # If huge_df is a list of dicts\n",
        "        model_df = [row for row in huge_df if not _excluded(row)]\n",
        "\n",
        "    tokenizer = token_limits[model][\"tokenizer\"]\n",
        "    token_limit = token_limits[model][\"tokens\"]\n",
        "\n",
        "    # We'll store final chunking here\n",
        "    all_batches = []\n",
        "    current_batch = []  # will hold a list of files\n",
        "    current_file = []   # will hold a list of question dicts\n",
        "\n",
        "    # Track how many tokens we've accumulated in *this batch* so far\n",
        "    batch_token_count = 0\n",
        "\n",
        "    # We'll enforce a max of  per 500 lines\n",
        "    FILE_LINE_LIMIT = 500\n",
        "    # Overhead tokens, to account for batch having unclear token limits inconsistent with the website\n",
        "    OVERHEAD_TOKENS = 5000\n",
        "\n",
        "    for idx, row in tqdm(\n",
        "        model_df.iterrows(),\n",
        "        total=len(model_df),\n",
        "        desc=f\"Chunking for {model}\"\n",
        "    ):\n",
        "        # print(row)\n",
        "        question = build_question(row, model)\n",
        "\n",
        "        # Token-count estimate:\n",
        "        messages = question[\"body\"][\"messages\"]\n",
        "        current_count = (\n",
        "            len(tokenizer.encode(messages[0][\"content\"])) +\n",
        "            len(tokenizer.encode(messages[1][\"content\"]))\n",
        "        )\n",
        "\n",
        "        # 1) Check if adding this question would exceed the batch token limit\n",
        "        if batch_token_count + OVERHEAD_TOKENS + current_count >= token_limit:\n",
        "            # First finalize the current file if it isn't empty\n",
        "            if current_file:\n",
        "                current_batch.append(current_file)\n",
        "\n",
        "            # Then finalize this entire batch\n",
        "            if current_batch:\n",
        "                all_batches.append(current_batch)\n",
        "\n",
        "            # Start a new batch\n",
        "            current_batch = []\n",
        "            current_file = [question]\n",
        "            batch_token_count = current_count  # reset + current question's tokens\n",
        "\n",
        "        # 2) Otherwise, check if adding this question would exceed the file line limit\n",
        "        elif len(current_file) >= FILE_LINE_LIMIT:\n",
        "            # finalize current_file in the current batch\n",
        "            current_batch.append(current_file)\n",
        "            # start a new file\n",
        "            current_file = [question]\n",
        "            batch_token_count += current_count\n",
        "\n",
        "        # 3) Otherwise, we can safely add the question to the current file\n",
        "        else:\n",
        "            current_file.append(question)\n",
        "            batch_token_count += current_count\n",
        "\n",
        "    # Finalize any leftover questions in the last file/batch\n",
        "    if current_file:\n",
        "        current_batch.append(current_file)\n",
        "    if current_batch:\n",
        "        all_batches.append(current_batch)\n",
        "\n",
        "    # Store chunked structure for this model\n",
        "    results[model] = all_batches\n",
        "\n",
        "# Make directories for the output\n",
        "os.makedirs(\"output\", exist_ok=True)  # root output folder\n",
        "\n",
        "# loop through the models and their batches\n",
        "for model_name, batches in results.items():\n",
        "    # Create a folder for the model\n",
        "    model_folder = os.path.join(\"output\", model_name)\n",
        "    os.makedirs(model_folder, exist_ok=True)\n",
        "\n",
        "    # A 'batches' subfolder within the model folder\n",
        "    batches_folder = os.path.join(model_folder, \"batches\")\n",
        "    os.makedirs(batches_folder, exist_ok=True)\n",
        "\n",
        "    # For each batch, write subfolders and JSONL files\n",
        "    for batch_idx, batch_files in enumerate(batches, start=1):\n",
        "        # batch_files is a list of \"files\"; each \"file\" is a list of question dicts\n",
        "        batch_folder = os.path.join(batches_folder, f\"batch{batch_idx}\")\n",
        "        os.makedirs(batch_folder, exist_ok=True)\n",
        "\n",
        "        for file_idx, file_questions in enumerate(batch_files, start=1):\n",
        "            file_path = os.path.join(batch_folder, f\"jsonlfile{file_idx}.jsonl\")\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                for q in file_questions:\n",
        "                    f.write(json.dumps(q) + \"\\n\")\n",
        "\n",
        "print(\"All done! Folder structure created in ./output\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## start batching proccess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrkkU7H7glP1"
      },
      "outputs": [],
      "source": [
        "# Poll every minute\n",
        "POLL_INTERVAL_SECONDS = 60\n",
        "\n",
        "def find_existing_batch_with_metadata(jsonl_file: str, batch_folder: str, model_name: str):\n",
        "    \"\"\"\n",
        "    Searches all existing batches for one whose metadata indicates\n",
        "    it's for the same model/batch folder/jsonl file AND\n",
        "    whose status is not in a 'failed' or 'expired' state.\n",
        "\n",
        "    Returns the batch object if found, or None if not found or if found but is in a \"failed/expired\" state.\n",
        "    \"\"\"\n",
        "    # NOTE: For production, you might want pagination or caching if you have a lot of batches.\n",
        "    existing_batches = client.batches.list()\n",
        "    for b in existing_batches.data:\n",
        "        # Example of matching by metadata\n",
        "        md = b.metadata or {}\n",
        "        if (md.get(\"model\") == model_name and\n",
        "            md.get(\"batch_folder\") == batch_folder and\n",
        "            md.get(\"filename\") == jsonl_file):\n",
        "            # Check if it's in a terminal \"bad\" state vs. \"good\" or in-progress state\n",
        "            if b.status in [\"failed\", \"expired\", \"cancelled\"]:\n",
        "                # It's an old or failed job, we can re-run\n",
        "                return None\n",
        "            else:\n",
        "                # It's either in progress, finalizing, or completed\n",
        "                return b\n",
        "    return None\n",
        "\n",
        "\n",
        "def process_model_folders_in_memory(model_name: str, root_output_dir: str = \"output\"):\n",
        "    \"\"\"\n",
        "    For each model:\n",
        "      1) Find output/<model>/batches/<batch1>, <batch2>, ...\n",
        "      2) For each <batchX> folder:\n",
        "         - Gather all .jsonl files\n",
        "         - Check if there's already a corresponding batch with the same metadata.\n",
        "           * If yes and it's in_progress or completed, skip creation\n",
        "           * If no or it's failed, create a new batch\n",
        "         - Collect all relevant batch objects (both newly created and re-used) in a list\n",
        "         - Poll them until they're all in a terminal state\n",
        "         - Finally, retrieve their in-memory results (output file + error file), parse them,\n",
        "           and store them in a Python data structure instead of writing to disk.\n",
        "    \"\"\"\n",
        "    # Structure: {\n",
        "    #   batch_folder_name: {\n",
        "    #       jsonl_file_name: {\n",
        "    #           \"batch_id\": \"...\",\n",
        "    #           \"status\": \"...\",\n",
        "    #           \"output_data\": [...],\n",
        "    #           \"error_data\": [...]\n",
        "    #       },\n",
        "    #       ...\n",
        "    #   },\n",
        "    #   ...\n",
        "    # }\n",
        "    model_results = {}\n",
        "\n",
        "    # Prepare the paths for output folders\n",
        "    model_path = os.path.join(root_output_dir, model_name)\n",
        "    batches_path = os.path.join(model_path, \"batches\")\n",
        "    if not os.path.isdir(batches_path):\n",
        "        print(f\"[{model_name}] No 'batches' folder found. Skipping.\")\n",
        "        return model_results\n",
        "\n",
        "    # Sort subfolders so you process them in a predictable order\n",
        "    batch_folder_list = sorted(\n",
        "        d for d in os.listdir(batches_path)\n",
        "        if os.path.isdir(os.path.join(batches_path, d))\n",
        "    )\n",
        "\n",
        "    for batch_folder in batch_folder_list:\n",
        "        folder_path = os.path.join(batches_path, batch_folder)\n",
        "        jsonl_files = sorted(\n",
        "            f for f in os.listdir(folder_path) if f.endswith(\".jsonl\")\n",
        "        )\n",
        "        print(f\"[{model_name}] Processing folder {batch_folder} with {len(jsonl_files)} files.\")\n",
        "\n",
        "        # Keep track of the batch objects we will poll\n",
        "        batch_jobs = []  # list of (jsonl_file, batch_id)\n",
        "        model_results[batch_folder] = {}\n",
        "\n",
        "        # 1) Send them all at once\n",
        "        for jsonl_file in jsonl_files:\n",
        "            # Check if there's an existing batch with this metadata\n",
        "            existing_batch = find_existing_batch_with_metadata(jsonl_file, batch_folder, model_name)\n",
        "            if existing_batch:\n",
        "                # Re-use the existing batch\n",
        "                batch_id = existing_batch.id\n",
        "                print(f\"[{model_name} -> {batch_folder}] Found existing batch {batch_id} for {jsonl_file}, status={existing_batch.status}\")\n",
        "                batch_jobs.append((jsonl_file, batch_id))\n",
        "                model_results[batch_folder][jsonl_file] = {\n",
        "                    \"batch_id\": batch_id,\n",
        "                    \"status\": existing_batch.status,\n",
        "                    \"output_data\": None,\n",
        "                    \"error_data\": None\n",
        "                }\n",
        "                continue\n",
        "\n",
        "            # If not found or it's in a fail/expired state, create a new one\n",
        "            file_path = os.path.join(folder_path, jsonl_file)\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                uploaded_file = client.files.create(file=f, purpose=\"batch\")\n",
        "            uploaded_file_id = uploaded_file.id\n",
        "            print(f\"[{model_name} -> {batch_folder}] Created file {uploaded_file_id} for {jsonl_file}\")\n",
        "\n",
        "            # Create the batch with metadata so we can detect it in the future\n",
        "            created_batch = client.batches.create(\n",
        "                input_file_id=uploaded_file_id,\n",
        "                endpoint=\"/v1/chat/completions\",\n",
        "                completion_window=\"24h\",\n",
        "                metadata={\n",
        "                    \"model\": model_name,\n",
        "                    \"batch_folder\": batch_folder,\n",
        "                    \"filename\": jsonl_file\n",
        "                }\n",
        "            )\n",
        "            batch_id = created_batch.id\n",
        "            print(f\"[{model_name} -> {batch_folder}] Started Batch {batch_id} for {jsonl_file} (status={created_batch.status})\")\n",
        "\n",
        "            batch_jobs.append((jsonl_file, batch_id))\n",
        "            # Initialize results structure\n",
        "            model_results[batch_folder][jsonl_file] = {\n",
        "                \"batch_id\": batch_id,\n",
        "                \"status\": created_batch.status,\n",
        "                \"output_data\": None,\n",
        "                \"error_data\": None\n",
        "            }\n",
        "\n",
        "        # 2) Poll all newly (or previously) created batches in this folder until they're done\n",
        "        if batch_jobs:\n",
        "            print(f\"[{model_name} -> {batch_folder}] Polling {len(batch_jobs)} batch jobs until completion...\")\n",
        "\n",
        "        while True:\n",
        "            if not batch_jobs:\n",
        "                break  # No jobs to poll\n",
        "            time.sleep(POLL_INTERVAL_SECONDS)\n",
        "\n",
        "            all_finished = True\n",
        "            for (jsonl_file, batch_id) in batch_jobs:\n",
        "                batch_status = client.batches.retrieve(batch_id)\n",
        "                status_str = batch_status.status\n",
        "                # Update the in-memory structure with latest status\n",
        "                model_results[batch_folder][jsonl_file][\"status\"] = status_str\n",
        "\n",
        "                if status_str not in [\"completed\", \"failed\", \"expired\", \"cancelled\"]:\n",
        "                    print(f\"[{model_name} -> {batch_folder}] Batch {batch_id} for {jsonl_file} is {status_str}...\")\n",
        "                    all_finished = False\n",
        "\n",
        "            if all_finished:\n",
        "                print(f\"[{model_name} -> {batch_folder}] All batch jobs done.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"[{model_name} -> {batch_folder}] Checking again in 10 minutes...\")\n",
        "\n",
        "        # 3) Retrieve results for each batch, parse in memory (no saving to disk)\n",
        "        for (jsonl_file, batch_id) in batch_jobs:\n",
        "            batch_status = client.batches.retrieve(batch_id)\n",
        "            final_state = batch_status.status\n",
        "\n",
        "            if final_state == \"completed\":\n",
        "                out_list = []\n",
        "                err_list = []\n",
        "\n",
        "                # Collect output\n",
        "                if batch_status.output_file_id:\n",
        "                    out_file_response = client.files.content(batch_status.output_file_id)\n",
        "                    # Each line is a JSON object; parse them\n",
        "                    for line in out_file_response.text.strip().split(\"\\n\"):\n",
        "                        if line.strip():\n",
        "                            out_list.append(json.loads(line))\n",
        "\n",
        "                # Collect errors\n",
        "                if batch_status.error_file_id:\n",
        "                    err_file_response = client.files.content(batch_status.error_file_id)\n",
        "                    for line in err_file_response.text.strip().split(\"\\n\"):\n",
        "                        if line.strip():\n",
        "                            err_list.append(json.loads(line))\n",
        "\n",
        "                model_results[batch_folder][jsonl_file][\"output_data\"] = out_list\n",
        "                model_results[batch_folder][jsonl_file][\"error_data\"] = err_list\n",
        "            else:\n",
        "                # Could be \"failed\", \"expired\", or \"cancelled\"\n",
        "                # If there's an error file, read it\n",
        "                err_list = []\n",
        "                if batch_status.error_file_id:\n",
        "                    err_file_response = client.files.content(batch_status.error_file_id)\n",
        "                    for line in err_file_response.text.strip().split(\"\\n\"):\n",
        "                        if line.strip():\n",
        "                            err_list.append(json.loads(line))\n",
        "\n",
        "                model_results[batch_folder][jsonl_file][\"error_data\"] = err_list\n",
        "\n",
        "    print(f\"[{model_name}] All batch folders complete.\")\n",
        "    return model_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    models_to_run = [\n",
        "        \"gpt-4o-mini\",\n",
        "        \"gpt-4o-2024-08-06\",\n",
        "        \"gpt-3.5-turbo-1106\"\n",
        "    ]\n",
        "\n",
        "    # We'll process each model in its own thread, concurrently\n",
        "    final_all_models_results = {}\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(models_to_run)) as executor:\n",
        "        future_to_model = {\n",
        "            executor.submit(process_model_folders_in_memory, m): m\n",
        "            for m in models_to_run\n",
        "        }\n",
        "        for future in concurrent.futures.as_completed(future_to_model):\n",
        "            model = future_to_model[future]\n",
        "            try:\n",
        "                model_results = future.result()\n",
        "                final_all_models_results[model] = model_results\n",
        "            except Exception as e:\n",
        "                print(f\"[{model}] ERROR: {e}\")\n",
        "\n",
        "    # At this point, final_all_models_results has the in-memory\n",
        "    # data for each model's batch runs.\n",
        "    print(\"All done!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
